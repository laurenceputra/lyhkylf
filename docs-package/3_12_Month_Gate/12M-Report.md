# LYHKYLF Fund: Month 12 Decision Memo (Template)
*(For charity/grantee to submit to the grant panel. Copy, fill, and keep responses concise. Use aggregate data only.)*

## 0) Cover page
- Grant / Program name:
- Grantee (legal entity):
- Program lead (name, role, phone/email):
- Reporting period (Month 0 to Month 12 dates):
- Cohort/cycle structure (e.g., rolling intake, 2 cycles of 6 months, etc.):
- Target population (age range, eligibility rule used):
- Delivery sites / channels (general description, no youth identifiers):
- Total grant amount and spend-to-date:
- Requested decision at Month 12: **Renew / Renew-with-conditions / Pivot / Stop / Graduate**

---

## 1) Executive summary (one page max)
### 1.1 Your recommended decision and why
**Recommended decision:**  
**Rationale (3–5 bullets):**
-  
-  
-  

### 1.2 What changed since Month 6 (baseline gate)
- Major delivery changes:
- Measurement changes (if any; explain why):
- Key learning(s) that altered your theory of change:

### 1.3 Topline numbers (aggregate)
| Metric | Value | Notes |
|---|---:|---|
| Youth enrolled (cumulative) |  |  |
| Youth active at Month 12 |  |  |
| Youth completed cycle(s) |  | Define “completed” |
| Median attendance / dosage |  | Include distribution note |
| Retention rate |  |  |
| Safeguarding incidents |  | Count + severity summary |
| Primary Outcome movement |  | Pre vs latest |
| Mechanism movement |  | Early vs later |

### 1.4 Panel “decision anchors” (tick and justify)
- [ ] Delivery is strong enough to interpret results (fidelity, retention, safety)
- [ ] There are benefit signals on the Primary Outcome
- [ ] Mechanism evidence plausibly links to outcome
- [ ] Comparison logic exists **or** is not feasible and you strengthened pre/post + mechanism evidence
- [ ] Unit economics are interpretable and you have a cost-down plan
- [ ] You selected a handoff lane and have a credible Month 24 path

---

## 2) Primaries recap (locked at Month 6)
### 2.1 Primary Outcome (Trajectory Lever)
- Name:
- Operational definition (behavioral/admin anchor):
- Unit of measure:
- Measurement cadence:
- Durability definition (what counts as “sustained” improvement):
- Data source (attendance logs, admin data, rubric, etc.):
- Known limitations / missingness:

### 2.2 Primary Mechanism Indicator (earliest observable proof)
- Name:
- Why this is the earliest proof the theory is firing:
- Operational definition:
- Measurement cadence:
- Instrument/rubric (attach if new or changed):
- Data quality checks used:

### 2.3 Secondary indicators (optional, keep lightweight)
List only what you actually used for learning/decisions:
-  
-  

---

## 3) Delivery and fidelity (is the program being delivered as intended?)
### 3.1 What was delivered vs plan
- Planned model (Month 0 plan, 5 bullets max):
- Delivered model (what actually happened):
- Deviations and why (include what you tried, what you stopped):

### 3.2 Attendance / dosage (required)
Provide distribution, not only averages.
| Segment | N | Median sessions | P25–P75 | Notes |
|---|---:|---:|---:|---|
| All enrolled |  |  |  |  |
| Completers |  |  |  |  |
| Dropouts |  |  |  |  |

### 3.3 Retention / completion (required)
- Define “retained” and “completed”:
- Retention by month/cycle (table or bullets):
- Top 5 dropout reasons (aggregate counts):
  1.
  2.
  3.
  4.
  5.

### 3.4 Implementation quality checks
- Supervision / training delivered (what, how often):
- Vendor/service-provider oversight (if any):
- Operational bottlenecks and how you mitigated them:

---

## 4) Results: Primary Outcome (benefit signals)
### 4.1 Data used (aggregate; no identifiers)
- Baseline window definition:
- Latest/Month 12 window definition:
- N with both baseline + follow-up:
- Missing data rate and why:

### 4.2 Primary Outcome movement (pre/post)
| Group | N | Baseline | Latest | Change | Interpretation |
|---|---:|---:|---:|---:|---|
| All with baseline+follow-up |  |  |  |  |  |
| High dosage (define) |  |  |  |  |  |
| Low dosage (define) |  |  |  |  |  |

### 4.3 Meaningfulness vs durability definition
- Does the observed change meet your durability threshold? Why / why not?
- If not yet durable, what evidence suggests it could become durable by Month 24?

### 4.4 Alternative explanations (required)
List the 2–4 most plausible alternative explanations and what you did (or can do) to test/rule out:
- Alternative explanation 1:
  - What evidence you have:
  - What evidence is missing:
- Alternative explanation 2:
  -  

---

## 5) Mechanism evidence and linkage (why you think change is happening)
### 5.1 Mechanism movement over time
| Timepoint | N | Mechanism score/metric | Notes |
|---|---:|---:|---|
| Early (define) |  |  |  |
| Mid (optional) |  |  |  |
| Late (define) |  |  |  |

### 5.2 Plausible linkage to the Primary Outcome (choose what you can support)
Tick any you can demonstrate with your data, then summarize evidence:
- [ ] Temporal ordering (mechanism moved before outcome)
- [ ] Dose-response (more mechanism movement aligns with more outcome movement)
- [ ] Observable anchor stories (aggregate patterns with rubric evidence, no identifiers)
- [ ] Comparison group shows weaker/no movement (if applicable)

**Evidence summary (5–10 bullets max):**
-  
-  

### 5.3 What you changed because of mechanism evidence
- What did you adjust in delivery based on mechanism signals?
- What did you stop doing?

---

## 6) Comparison logic (if feasible)
### 6.1 Evaluation design used
- [ ] None (justify)
- [ ] Pre/post only
- [ ] Matched comparison
- [ ] Stepped rollout
- [ ] Other (describe)

### 6.2 If you used a comparison group
- Who is comparison group and how selected:
- Key baseline differences that remain:
- Results summary (same tables as Sections 4 and 5, for both groups):

### 6.3 If you did not use comparison (required justification)
- Why infeasible in your context:
- What you did to strengthen inference (e.g., better baselines, tighter mechanism, stepped rollout plan for Year 2):

---

## 7) Unit economics and budget discipline
### 7.1 Spend-to-date and variance
| Budget line | Planned (Y1) | Actual to date | Variance | Why |
|---|---:|---:|---:|---|
| Program delivery |  |  |  |  |
| Vendor/service providers |  |  |  |  |
| Safeguarding/PDPA |  |  |  |  |
| Measurement/Evaluation |  |  |  |  |
| Overhead |  |  |  |  |

### 7.2 Unit economics (required)
- Cost per youth enrolled:
- Cost per active youth at Month 12:
- Cost per completer:
- Cost per youth per cycle (if cycles exist):
- Top 3 cost drivers and why:

### 7.3 Cost-down hypotheses for next 12 months (required)
Provide 2–5 concrete levers with expected magnitude:
| Lever | What changes | Est. savings | Risk to outcomes | Mitigation |
|---|---|---:|---|---|
|  |  |  |  |  |

### 7.4 Vendor / for-profit involvement (if any)
- Vendor roles and responsibilities (service-provider model confirmation):
- How you prevent “pass-through” control issues:
- Contract oversight approach:

### 7.5 Measurement burden check (required)
- Estimated % of grant resources spent on measurement (time + cost):
- If > 20%, justification and why it is still necessary:

---

## 8) Safeguarding (required, non-negotiable)
### 8.1 Safeguarding standard in place (tick and attach references)
- [ ] Child protection policy
- [ ] Mandatory reporting SOP
- [ ] Incident logging + escalation workflow
- [ ] Staff/volunteer training
- [ ] Screening/background checks where applicable
- [ ] Code of conduct and safe supervision practices
- [ ] Parental consent protocol
- [ ] Venue/activity risk assessment and emergency response plan
- [ ] Transport rules (if relevant)

### 8.2 Incidents (aggregate)
| Severity | Count | Median time to resolve | What changed because of this |
|---|---:|---:|---|
| Low |  |  |  |
| Medium |  |  |  |
| High |  |  |  |

### 8.3 Auto-stop trigger check (required)
- Any **unresolved high-risk incident** lasting > 7 days? **Yes / No**
- If yes: describe status, actions taken, and why it is unresolved (panel will treat as a stop trigger).

---

## 9) Data dignity and PDPA (required)
### 9.1 What data you collect (minimal by default)
- Direct identifiers collected (should be limited to name + phone for ops):
- Any additional fields collected (justify each):
- Any sensitive data collected (should be none; if any, explain and mitigation):

### 9.2 Consent and opt-out
- How consent is captured at enrollment:
- How opt-out works for non-essential measurement:
- Any issues encountered:

### 9.3 Reporting and access control
- Confirm the fund receives aggregate reporting only:
- Who has access to identifiable data:
- Data storage and security controls (brief):

### 9.4 Retention and deletion
- Raw data deletion date (max 12 months after program end):
- Aggregated data retention plan:

---

## 10) Handoff lane selection and Month 24 readiness (forward planning)
### 10.1 Selected handoff lane (required by Month 12)
Choose one and be specific:
- [ ] Government adoption
- [ ] Large charity / network replication
- [ ] School-based integration
- [ ] Vendor-market model with charity governance
- [ ] Other (define)

**Lane description (who would adopt, what they would adopt, why they would adopt):**
- Intended adopter(s) and why they care:
- What “handoff” looks like in practice (assets, training, policy change, procurement, etc.):

### 10.2 What is missing for handoff readiness
| Readiness element | Current status | Gap | Next 12 months action |
|---|---|---|---|
| Playbook maturity |  |  |  |
| Unit economics stability |  |  |  |
| Evidence snapshot strength |  |  |  |
| Implementation constraints |  |  |  |

### 10.3 Fund co-development asks (what you need from Month 12 onward)
- Evidence narrative support needed:
- Partner introductions or lane validation needed:
- Packaging help needed (playbook, unit economics, evaluation design):

---

## 11) Next 12 months plan (go / pivot conditions)
### 11.1 Key hypotheses for Year 2 (2–4 max)
Write them as testable statements.
1.
2.
3.
4.

### 11.2 Planned changes (if renewed)
- Delivery changes:
- Mechanism strengthening changes:
- Measurement upgrades (keep lightweight):

### 11.3 What “success” looks like by Month 24 (decision thresholds)
Define 3–6 thresholds that would justify “continue / handoff-ready”:
- Primary Outcome durability threshold:
- Required mechanism threshold:
- Minimum retention/dosage threshold:
- Unit economics threshold:
- Safeguarding threshold (e.g., incident resolution SLA):

### 11.4 Pivot plan (if panel requests pivot)
If the panel recommends “pivot” or “renew-with-conditions,” propose:
- Pivot option A (what changes, expected effect, risks):
- Pivot option B (backup):

### 11.5 Stop plan (if panel requests stop)
- How you will wind down safely:
- What you will publish as learnings:
- What you will retain/delete (data and artifacts):

---

## 12) Publication-ready learning summary (one page, comms-safe)
*(No youth identifiers. No operationally sensitive safeguarding details. Use conservative claims.)*

### 12.1 Summary of what was tried
-  

### 12.2 What worked / what did not
- Worked:
- Did not:

### 12.3 Evidence level check (Claim Ladder)
Tick only what is true based on this memo:
- [ ] Intent-only (we tested, learned, published, packaged)
- [ ] After Month 6: primaries locked; early mechanism movement observed
- [ ] After Month 12: benefit signals and plausible linkage
- [ ] After Month 24: durability/repeatability and a handoff-ready package (not applicable yet)

### 12.4 Standard disclosures to include in external sharing
- Safeguarding and mandatory reporting are non-negotiable
- Minimal data collection; aggregate reporting; privacy-first

---

# Annexes (attach as needed)
A1. Data dictionary (definitions of each metric used)  
A2. Instruments/rubrics used (mechanism, outcome if applicable)  
A3. Budget detail and invoices summary (aggregate)  
A4. Incident log excerpt (aggregate, de-identified)  
A5. Playbook draft outline (if available)  
A6. Partner/stakeholder mapping for handoff lane  
